---
title: "Generative AI in Biotech"
date: 2024-11-14T18:09:00
draft: false
author: "Divya Ramamoorthy"
tags:
 - generative AI
#image: /images/profile_picture.jpg
description: "An overview of generative AI in biotechnology"
toc:
---

##

We're seeing applications of generative AI grow by the day - from advanced chatbots, to AI-generated images. But what actually is generative AI? This article aims to break down the core components of generative AI and demonstrate applications in the biotech field.

**Key concepts**: latent factors, self-supervised learning, next token prediction, de-noising

## What is Generative AI?

Generative AI is the use of a machine learning model to create new data - this can be new sentences, new images, new molecules (and so on!). There are a variety of machine learning architectures that underpin GenAI models, but in general, they follow two stages: 1) Learn the patterns in data (our 'model') 2) Sample from this model to create new data points.

### Stage 1: Learn the patterns in data

We want our model to identify the underlying properties that influence our data (these are called **latent factors**) and understand how these properties interact. Latent factors are representations of your data that aim to capture higher-level properties. For example, if you asked a model to draw an image in "Monet's style" you want the model to know that Monet was an impressionist artist, and impressionism correlates with blurry swatches of color - these are properties that define the generation but are not directly labeled from the inputs. An example in biology: if you asked a model to generate a new IgG molecule, you would want the model to capture that IgG is a subclass of antibodies - which comes with defined structural patterns.  

A model learns these latent factors through a process called **self-supervised  learning**. This training method focuses on taking an input data, corrupting it in some way, and learning how to recreate the initial dataset. The model evaluates how well the missing data was reconstructed, and optimizes its parameters until it is able to reliably construct the data. The process of learning how to 'fill in the gaps' helps a model learn the generalizable properties of inputs.

![latent_factors](/images/101624_genai_latent_factors.png)

### Stage 2: Sample from our model to generate new data

Once we have learned the patterns in our dataset, we can then use that information to generate new data points. One interesting realization of the field is that generating data from scratch is difficult, but generating in small steps is much easier. For text, this can mean reconstructing the next word in a sentence is much easier than predicting the entire next sentence (this process is called **next token prediction**). For images, it can be easier to this first create a blurry outline of the image and then later fill in fine-details, in a process called **de-noising**.

We can also feed a model additional information to guide the type of sample that gets generated. This can look like providing a prompt for an image generator or a question to a chatbot. It can also look like giving the model a specific subclass of data that shares property of what you want to generate, and further training the model on that dataset (**fine-tuning**). In these cases, the first stage of the training teaches a model general properties of the type of data you want to create, and then the second stage teaches a model specific properties of the subclass you're interested in. For example, to generate a new antibody, one can start by training a model on a diverse set of protein sequences in the PDB (Protein Data Bank), and then fine-tuning that model specifically on other existing antibodies.

![sample_generation](/images/101624_genai_ntp_denoise.png)

## Architectures and Applications

We've skipped over a key part - how exactly do we learn the patterns of latent factors in our data? This depends on the type of data. For text generation, the field has focused on using a specific architecture called a Transformer to model and capture relationships in languages. For image generation, the field has centered on Diffusion models to generate realistic pictures.

Applications for biological problems tend to be problem-specific, with scientists typically adapting the architectures used in the text and imaging domain. Examples include models for protein design, small-molecule generation, and cell state representation.

We're at an exciting age in biotech - a time in which we have unprecedented access to large biological datasets, the computational advances in generative AI that allow us to efficiently ingest those datasets, and an ever-present need for faster drug development.

Are there any generative AI x biology applications that you're particularly excited about? Comment below!
